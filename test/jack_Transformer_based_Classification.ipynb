{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer_based_Classification_Spirit.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ygujDyznVK8y"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamksseo/NeuralLog/blob/main/test/jack_Transformer_based_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "se4OVzeBUFBU",
        "outputId": "ea2b03c8-e0a8-4480-c9da-121e149419d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ml_data/neurallog\n",
        "!ls\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "qAvwbLtbUOXu",
        "outputId": "5a63eafc-fa44-4f6f-b9e5-aab040844292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml_data/neurallog\n",
            "data  docs\t\t     LICENSE  neurallog  requirements.txt\n",
            "demo  hdfs_transformer.hdf5  logs     README.md  test\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy~=1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 25.2 MB/s \n",
            "\u001b[?25hCollecting scikit-learn~=0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting transformers~=4.19.2\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 44.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.4.0\n",
            "  Downloading tensorflow-2.4.4-cp37-cp37m-manylinux2010_x86_64.whl (394.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.5 MB 39 kB/s \n",
            "\u001b[?25hCollecting tf-models-official~=2.4.0\n",
            "  Downloading tf_models_official-2.4.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas~=1.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.3.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.24.2->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.24.2->-r requirements.txt (line 2)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.24.2->-r requirements.txt (line 2)) (1.1.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (5.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.19.2->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (0.37.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 55.6 MB/s \n",
            "\u001b[?25hCollecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 72.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.12)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 54.8 MB/s \n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (2.9.1)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.21.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.5.12)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.1.97)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.29.32)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.17.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.7.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.12.11)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (4.6.0.66)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (4.6.0)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (8.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official~=2.4.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.3.5->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.3.5->-r requirements.txt (line 6)) (2022.2.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.31.6)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.56.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (6.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers~=4.19.2->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.19.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.19.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers~=4.19.2->-r requirements.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow~=2.4.0->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.1.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (5.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.3.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official~=2.4.0->-r requirements.txt (line 5)) (1.9.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68721 sha256=99a1b221a6bb780db9443fed60dcccacffc1abe618b29bab4ba51b410dc09347\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, scikit-learn, h5py, gast, tokenizers, tensorflow, huggingface-hub, dataclasses, transformers, tf-models-official\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.2.0\n",
            "    Uninstalling absl-py-1.2.0:\n",
            "      Successfully uninstalled absl-py-1.2.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n",
            "  Attempting uninstall: tf-models-official\n",
            "    Found existing installation: tf-models-official 2.9.2\n",
            "    Uninstalling tf-models-official-2.9.2:\n",
            "      Successfully uninstalled tf-models-official-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow-text 2.9.0 requires tensorflow<2.10,>=2.9.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.4.4 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 dataclasses-0.6 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 huggingface-hub-0.9.1 numpy-1.19.5 scikit-learn-0.24.2 tensorflow-2.4.4 tensorflow-estimator-2.4.0 tf-models-official-2.4.0 tokenizers-0.12.1 transformers-4.19.4 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "dataclasses",
                  "gast",
                  "h5py",
                  "numpy",
                  "official",
                  "sklearn",
                  "tensorflow",
                  "typing_extensions",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwFyen9PamnP",
        "outputId": "1bb0922d-9439-411a-f6e3-655ce115fc84",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 29 23:34:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLFhbl0yi63A",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9702c9c7-173c-4983-cbee-8ba876b663db"
      },
      "source": [
        "pip install -q tf-models-official"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1 MB 27.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 116 kB 64.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 68.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 70.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.4 kB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 69.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 66.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 45.3 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx3lltd58zkg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYQGTcl_86xy",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from official.nlp import optimization"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95b_uEPLgSIq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqf3h3Sh88xN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1kFJDLP8bwe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Liu0qGrAVNxp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# II. Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PUKB02N_QTF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7taKfNmh8e5B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        " \n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM6iBfZP6U6C",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "class PositionEmbedding(layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embed_dim):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
        "        self.pos_encoding = positional_encoding(max_len,\n",
        "                                                embed_dim)\n",
        " \n",
        "    def call(self, x):\n",
        "        # x = self.token_emb(x)\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # print(maxlen)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        # positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions = self.pos_emb(positions)\n",
        "        # print(x.shape, positions.shape)\n",
        "        # x = self.token_emb(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mziDCzBZ8g2q",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "embed_dim = 768  # Embedding size for each token\n",
        "num_heads = 12  # Number of attention heads\n",
        "ff_dim = 2048  # Hidden layer size in feed forward network inside transformer\n",
        "max_len = 75\n",
        "num_layers = 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC3Duqe08qkZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def transformer_classifer(input_size, loss_object, optimizer, dropout=0.1):\n",
        "    inputs = layers.Input(shape=(max_len, embed_dim))\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "    embedding_layer = PositionEmbedding(100, 2000, embed_dim)\n",
        "    # print(inputs.shape)\n",
        "    x = embedding_layer(inputs)\n",
        "    # print(x.shape)\n",
        "    x = transformer_block(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss=loss_object, metrics=['accuracy'],\n",
        "                  optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abg-kaEbXYKM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training/Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3y6Us-99Jyk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "class BatchGenerator(Sequence):\n",
        " \n",
        "    def __init__(self, X, Y, batch_size):\n",
        "        self.X, self.Y = X, Y\n",
        "        self.batch_size = batch_size\n",
        " \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.batch_size)\n",
        "        dummy = np.zeros(shape=(embed_dim,))\n",
        "        x = self.X[idx * self.batch_size:min((idx + 1) * self.batch_size, len(self.X))]\n",
        "        X = np.zeros((len(x), max_len, embed_dim))\n",
        "        Y = np.zeros((len(x), 2))\n",
        "        item_count = 0\n",
        "        for i in range(idx * self.batch_size, min((idx + 1) * self.batch_size, len(self.X))):\n",
        "            x = self.X[i]\n",
        "            if len(x) > max_len:\n",
        "                x = x[-max_len:]\n",
        "            x = np.pad(np.array(x), pad_width=((max_len - len(x), 0), (0, 0)), mode='constant',\n",
        "                       constant_values=0)\n",
        "            X[item_count] = np.reshape(x, [max_len, embed_dim])\n",
        "            Y[item_count] = self.Y[i]\n",
        "            item_count += 1\n",
        "        return X[:], Y[:, 0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hry6lv0psLS",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        " \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        " \n",
        "        self.warmup_steps = warmup_steps\n",
        " \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        " \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XZXMT8-9TtR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def train_generator(training_generator, validate_generator, num_train_samples, num_val_samples, batch_size,\n",
        "                      epoch_num, model_name=None):\n",
        "  \n",
        "    # learning_rate = CustomSchedule(768)\n",
        " \n",
        "    # optim = tf.keras.optimizers.Adam(learning_rate)\n",
        "    \n",
        "    optim = Adam()\n",
        "    epochs = epoch_num\n",
        "    steps_per_epoch = num_train_samples \n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        " \n",
        "    init_lr = 3e-4\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                              num_train_steps=num_train_steps,\n",
        "                                              num_warmup_steps=num_warmup_steps,\n",
        "                                              optimizer_type='adamw')\n",
        "    \n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        " \n",
        "    model = transformer_classifer(768, loss_object, optimizer)\n",
        " \n",
        "    # model.load_weights(\"hdfs_transformer.hdf5\")\n",
        " \n",
        "    print(model.summary())\n",
        " \n",
        "    # checkpoint\n",
        "    filepath = model_name\n",
        "    checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='val_accuracy',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='max',\n",
        "                                 save_weights_only=True)\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n",
        "        baseline=None, restore_best_weights=True\n",
        "    )\n",
        "    callbacks_list = [checkpoint, early_stop]\n",
        "    \n",
        "    # class_weight = {0: 245., 1: 1.}\n",
        " \n",
        "    model.fit_generator(generator=training_generator,\n",
        "                        steps_per_epoch=int(num_train_samples / batch_size),\n",
        "                        epochs=epoch_num,\n",
        "                        verbose=1,\n",
        "                        validation_data=validate_generator,\n",
        "                        validation_steps=int(num_val_samples / batch_size),\n",
        "                        workers=16,\n",
        "                        max_queue_size=32,\n",
        "                        callbacks=callbacks_list,\n",
        "                        shuffle=True\n",
        "                        # class_weight=class_weight\n",
        "                        )\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_7UPOMJ9HBQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def train(X, Y, epoch_num, batch_size, tx, ty, model_file=None):\n",
        "    X, Y = shuffle(X, Y)\n",
        "    n_samples = len(X)\n",
        "    train_x, train_y = X[:int(n_samples * 90 / 100)], Y[:int(n_samples * 90 / 100)]\n",
        "    val_x, val_y = X[int(n_samples * 90 / 100):], Y[int(n_samples * 90 / 100):]\n",
        "    \n",
        "    training_generator, num_train_samples = BatchGenerator(train_x, train_y, batch_size), len(train_x)\n",
        "    validate_generator, num_val_samples = BatchGenerator(val_x, val_y, batch_size), len(val_x)\n",
        " \n",
        "    print(\"Number of training samples: {0} - Number of validating samples: {1}\".format(num_train_samples,\n",
        "                                                                                       num_val_samples))\n",
        " \n",
        "    model = train_generator(training_generator, validate_generator, num_train_samples, num_val_samples, batch_size,\n",
        "                              epoch_num, model_name=model_file)\n",
        "    test_model(model, tx, ty, batch_size)\n",
        " \n",
        " \n",
        "def test_model(model, x, y, batch_size):\n",
        "    x, y = shuffle(x, y)\n",
        "    x, y = x[: len(x) // batch_size * batch_size], y[: len(y) // batch_size * batch_size]\n",
        "    test_loader = BatchGenerator(x, y, batch_size)\n",
        "    prediction = model.predict_generator(test_loader, steps=(len(x) // batch_size), workers=16, max_queue_size=32,\n",
        "                                         verbose=1)\n",
        "    prediction = np.argmax(prediction, axis=1)\n",
        "    y = y[:len(prediction)]\n",
        "    report = classification_report(np.array(y), prediction)\n",
        "    print(report)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BKGP36V9A1i",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFM5sqXJ8tHg",
        "outputId": "96f384e7-8868-427c-a501-9062f1b21c58",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "#/content/drive/MyDrive/ml_data/neurallog/\n",
        "with open(\"data/embeddings/HDFS/neural-train.pkl\", mode=\"rb\") as f:\n",
        "    (x_tr, y_tr) = pickle.load(f)\n",
        "x_tr, y_tr = shuffle(x_tr, y_tr)\n",
        "print(Counter(y_tr))\n",
        "with open(\"data/embeddings/HDFS/neural-test.pkl\", mode=\"rb\") as f:\n",
        "    (x_te, y_te) = pickle.load(f)\n",
        "print(Counter(y_te))\n",
        "print(\"Data loaded\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 446559, 1: 13489})\n",
            "Counter({0: 111664, 1: 3349})\n",
            "Data loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qC3cZmR9F26",
        "outputId": "60b8b3aa-21c3-4c85-ba9e-a51f4c1e92aa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "train(x_tr, y_tr, 20, 64, x_te, y_te, \"hdfs_transformer.hdf5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 414043 - Number of validating samples: 46005\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 75, 768)]         0         \n",
            "                                                                 \n",
            " position_embedding (Positio  (None, 75, 768)          0         \n",
            " nEmbedding)                                                     \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, 75, 768)          31491584  \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 768)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                24608     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,516,258\n",
            "Trainable params: 31,516,258\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9740\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98666, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1837s 283ms/step - loss: 0.1135 - accuracy: 0.9740 - val_loss: 0.0492 - val_accuracy: 0.9867\n",
            "Epoch 2/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9931\n",
            "Epoch 2: val_accuracy improved from 0.98666 to 0.99682, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1829s 283ms/step - loss: 0.0314 - accuracy: 0.9931 - val_loss: 0.0155 - val_accuracy: 0.9968\n",
            "Epoch 3/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9974\n",
            "Epoch 3: val_accuracy improved from 0.99682 to 0.99804, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1825s 282ms/step - loss: 0.0135 - accuracy: 0.9974 - val_loss: 0.0096 - val_accuracy: 0.9980\n",
            "Epoch 4/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9987\n",
            "Epoch 4: val_accuracy improved from 0.99804 to 0.99943, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1828s 282ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0019 - val_accuracy: 0.9994\n",
            "Epoch 5/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
            "Epoch 5: val_accuracy improved from 0.99943 to 0.99952, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1830s 283ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
            "Epoch 6/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
            "Epoch 6: val_accuracy improved from 0.99952 to 0.99976, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1829s 283ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 9.4482e-04 - val_accuracy: 0.9998\n",
            "Epoch 7/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
            "Epoch 7: val_accuracy improved from 0.99976 to 0.99978, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1827s 282ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 7.5673e-04 - val_accuracy: 0.9998\n",
            "Epoch 8/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
            "Epoch 8: val_accuracy improved from 0.99978 to 0.99985, saving model to hdfs_transformer.hdf5\n",
            "6469/6469 [==============================] - 1825s 282ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 8.6100e-04 - val_accuracy: 0.9998\n",
            "Epoch 9/20\n",
            "6469/6469 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
            "Epoch 9: val_accuracy did not improve from 0.99985\n",
            "6469/6469 [==============================] - 1833s 283ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 5.8491e-04 - val_accuracy: 0.9998\n",
            "Epoch 10/20\n",
            " 321/6469 [>.............................] - ETA: 27:43 - loss: 3.3102e-04 - accuracy: 0.9999"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdZWtvaBYSFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}